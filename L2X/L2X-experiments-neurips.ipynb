{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if you want to only reserve memory on a single GPU\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Input, GlobalMaxPooling1D, Multiply, Lambda, Embedding, Dense, Dropout, Activation\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# data is available here: http://people.csail.mit.edu/taolei/beer/\n",
    "# place the data in the subfolder \"data\"\n",
    "# here we toggle between aspects\n",
    "aspect = 1\n",
    "input_path_train = \"data/reviews.aspect\" + str(aspect) + \".train.txt\"\n",
    "input_path_validation = \"data/reviews.aspect\" + str(aspect) + \".heldout.txt\"\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dictionary mapping words to their IDs\n",
    "word_to_id = dict()\n",
    "token_id_counter = 3\n",
    "\n",
    "\n",
    "with open(input_path_train) as fin:\n",
    "    for line in fin:\n",
    "        y, sep, text = line.partition(\"\\t\")\n",
    "        token_list = text.split(\" \")\n",
    "        for token in token_list:\n",
    "            if token not in word_to_id:\n",
    "                word_to_id[token] = token_id_counter\n",
    "                token_id_counter = token_id_counter + 1\n",
    "        \n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "\n",
    "X_train_list = []\n",
    "Y_train_list = []\n",
    "# now we iterate again to assign IDs\n",
    "with open(input_path_train) as fin:\n",
    "    for line in fin:\n",
    "        y, sep, text = line.partition(\"\\t\")\n",
    "        token_list = text.split(\" \")\n",
    "        tokenid_list = [word_to_id[token] for token in token_list]\n",
    "        X_train_list.append(tokenid_list)\n",
    "        \n",
    "        # extract the normalized [0,1] value for the aspect\n",
    "        y = [ float(v) for v in y.split() ]\n",
    "        Y_train_list.append(y[aspect])\n",
    "\n",
    "#print(y_list)        \n",
    "X_train = np.asarray(X_train_list)\n",
    "Y_train = np.asarray(Y_train_list)\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=350)\n",
    "\n",
    "print(\"Loading heldout data...\")\n",
    "X_val_list = []\n",
    "Y_val_list = []\n",
    "# now we iterate again to assign IDs\n",
    "with open(input_path_validation) as fin:\n",
    "    for line in fin:\n",
    "        y, sep, text = line.partition(\"\\t\")\n",
    "        token_list = text.split(\" \")\n",
    "        tokenid_list = [word_to_id.get(token, 2) for token in token_list]\n",
    "        X_val_list.append(tokenid_list)\n",
    "        \n",
    "        # extract the normalized [0,1] value for the aspect\n",
    "        y = [ float(v) for v in y.split() ]\n",
    "        Y_val_list.append(y[aspect])\n",
    "\n",
    "#print(y_list)        \n",
    "X_val_both = np.asarray(X_val_list)\n",
    "Y_val_both = np.asarray(Y_val_list)\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_val_both = sequence.pad_sequences(X_val_both, maxlen=350)\n",
    "\n",
    "print(X_train.shape)\n",
    "#print(X_train[10])\n",
    "#print(Y_train[0:10])\n",
    "#print(Y_val_both[0:10])\n",
    "#print(Y_train[0:100])\n",
    "#print(token_id_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters:\n",
    "max_features = token_id_counter+1\n",
    "maxlen = 350\n",
    "batch_size = 40\n",
    "embedding_dims = 200\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 20\n",
    "select_k = 10 # Number of selected words by the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell loads the word embeddings from the external data\n",
    "embeddings_index = {}\n",
    "f = open(\"data/review+wiki.filtered.200.txt\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_to_id) + 1, 200))\n",
    "for word, i in word_to_id.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "def gumbel_keys(w):\n",
    "    # sample some gumbels\n",
    "    uniform = tf.random.uniform(\n",
    "        tf.shape(w),\n",
    "        minval=EPSILON,\n",
    "        maxval=1.0)\n",
    "    z = tf.math.log(-tf.math.log(uniform))\n",
    "    w = w + z\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_topk(w, k, t, separate=False):\n",
    "    khot_list = []\n",
    "    onehot_approx = tf.zeros_like(w, dtype=tf.float32)\n",
    "    for i in range(k):\n",
    "        khot_mask = tf.maximum(1.0 - onehot_approx, EPSILON)\n",
    "        w += tf.math.log(khot_mask)\n",
    "        onehot_approx = tf.nn.softmax(w / t, axis=-1)\n",
    "        khot_list.append(onehot_approx)\n",
    "    if separate:\n",
    "        return khot_list\n",
    "    else:\n",
    "        return tf.reduce_sum(khot_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_subset(w, k, t=0.1):\n",
    "    '''\n",
    "    Args:\n",
    "        w (Tensor): Float Tensor of weights for each element. In gumbel mode\n",
    "            these are interpreted as log probabilities\n",
    "        k (int): number of elements in the subset sample\n",
    "        t (float): temperature of the softmax\n",
    "    '''\n",
    "    w = gumbel_keys(w)\n",
    "    return continuous_topk(w, k, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleSubset(Layer):\n",
    "    \"\"\"\n",
    "    Layer for continuous approx of subset sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, tau0, k, **kwargs):\n",
    "        self.tau0 = tau0\n",
    "        self.k = k\n",
    "        super(SampleSubset, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, logits):\n",
    "        # logits: [BATCH_SIZE, d, 1]\n",
    "        logits = tf.squeeze(logits, 2)\n",
    "        samples = sample_subset(logits, self.k, self.tau0)\n",
    "\n",
    "        # Explanation Stage output.\n",
    "        threshold = tf.expand_dims(tf.nn.top_k(logits, self.k, sorted = True)[0][:,-1], -1)\n",
    "        discrete_logits = tf.cast(tf.greater_equal(logits,threshold),tf.float32)\n",
    "        output = K.in_train_phase(samples, discrete_logits)\n",
    "        return tf.expand_dims(output,-1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_precision(modelTestInput):\n",
    "    data = []\n",
    "    num_annotated_reviews = 0\n",
    "    with open(\"data/annotations.json\") as fin:\n",
    "        for line in fin:\n",
    "            item = json.loads(line)\n",
    "            data.append(item)\n",
    "            num_annotated_reviews = num_annotated_reviews + 1\n",
    "\n",
    "    selected_word_counter = 0\n",
    "    correct_selected_counter = 0\n",
    "\n",
    "    for anotr in range(num_annotated_reviews):\n",
    "        #print(anotr),\n",
    "        ranges = data[anotr][str(aspect)] # the aspect id\n",
    "        text_list = data[anotr]['x']\n",
    "        #print(ranges)\n",
    "        review_length = len(text_list)\n",
    "        #print(text_list)\n",
    "\n",
    "        list_test = []\n",
    "        tokenid_list = [word_to_id.get(token, 0) for token in text_list]\n",
    "        list_test.append(tokenid_list)\n",
    "\n",
    "        #print(list_test)\n",
    "        X_test_subset = np.asarray(list_test)\n",
    "        X_test_subset = sequence.pad_sequences(X_test_subset, maxlen=350)\n",
    "        #print(X_test_subset)\n",
    "\n",
    "        prediction = modelTestInput.predict(X_test_subset)\n",
    "        prediction = tf.squeeze(prediction, -1)\n",
    "        #print(np.count_nonzero(prediction[0]))\n",
    "\n",
    "        #print(prediction[0])\n",
    "        x_val_selected = prediction[0] * X_test_subset\n",
    "        #print(tf.cast(x_val_selected, tf.int32))\n",
    "\n",
    "        selected_words = np.vectorize(id_to_word.get)(x_val_selected)[0][-review_length:]\n",
    "        selected_nonpadding_word_counter = 0\n",
    "        \n",
    "        for i, w in enumerate(selected_words):\n",
    "            if w != '<PAD>': # we are nice to the L2X approach by only considering selected non-pad tokens\n",
    "                selected_nonpadding_word_counter = selected_nonpadding_word_counter + 1\n",
    "                for r in ranges:\n",
    "                    rl = list(r)\n",
    "                    if i in range(rl[0], rl[1]):\n",
    "                        correct_selected_counter = correct_selected_counter + 1\n",
    "        # we make sure that we select at least 10 non-padding words\n",
    "        # if we have more than select_k non-padding words selected, we allow it but count that in\n",
    "        selected_word_counter = selected_word_counter + max(selected_nonpadding_word_counter, select_k)\n",
    "\n",
    "    return correct_selected_counter / selected_word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "####################L2X####################\n",
    "###########################################\n",
    "# Define various Keras layers.\n",
    "Mean = Lambda(lambda x: K.sum(x, axis = 1) / float(select_k), output_shape=lambda x: [x[0],x[2]])\n",
    "\n",
    "class Concatenate(Layer):\n",
    "    \"\"\"\n",
    "    Layer for concatenation. \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs): \n",
    "        super(Concatenate, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input1, input2 = inputs  \n",
    "        input1 = tf.expand_dims(input1, axis = -2) # [batchsize, 1, input1_dim] \n",
    "        dim1 = int(input2.get_shape()[1])\n",
    "        input1 = tf.tile(input1, [1, dim1, 1])\n",
    "        return tf.concat([input1, input2], axis = -1)\n",
    "\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        input_shape1, input_shape2 = input_shapes\n",
    "        input_shape = list(input_shape2)\n",
    "        input_shape[-1] = int(input_shape[-1]) + int(input_shape1[-1])\n",
    "        input_shape[-2] = int(input_shape[-2])\n",
    "        return tuple(input_shape)\n",
    "\n",
    "class Sample_Concrete(Layer):\n",
    "    \"\"\"\n",
    "    Layer for sample Concrete / Gumbel-Softmax variables. \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, tau0, k, **kwargs): \n",
    "        self.tau0 = tau0\n",
    "        self.k = k\n",
    "        super(Sample_Concrete, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, logits):   \n",
    "        # logits: [batch_size, d, 1]\n",
    "        logits_ = K.permute_dimensions(logits, (0,2,1))# [batch_size, 1, d]\n",
    "\n",
    "        d = int(logits_.get_shape()[2])\n",
    "        \n",
    "        uniform = tf.random.uniform(shape=tf.shape(logits_), minval=0.0, maxval = 1.0)\n",
    "        gumbel = - K.log(-K.log(uniform))\n",
    "        noisy_logits = (gumbel + logits_)/self.tau0\n",
    "        samples = K.softmax(noisy_logits)\n",
    "        samples = K.max(samples, axis = 1) \n",
    "        logits = tf.reshape(logits,[-1, d]) \n",
    "        threshold = tf.expand_dims(tf.nn.top_k(logits, self.k, sorted = True)[0][:,-1], -1)\n",
    "        discrete_logits = tf.cast(tf.greater_equal(logits,threshold),tf.float32)\n",
    "        \n",
    "        output = K.in_train_phase(samples, discrete_logits) \n",
    "        return tf.expand_dims(output,-1)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg  \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "class IMLESubsetkLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, k, _tau=1.0, _lambda=10.0):\n",
    "        super(IMLESubsetkLayer, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        self._tau = _tau\n",
    "        self._lambda = _lambda\n",
    "        self.samples = None\n",
    "        \n",
    "    def sample_gumbel(self, shape, eps=1e-20):\n",
    "        U = tf.random.uniform(shape, minval=0, maxval=1)\n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "        \n",
    "    def sample_discrete(self, logits):\n",
    "        gumbel_softmax_sample = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        threshold = tf.expand_dims(tf.nn.top_k(gumbel_softmax_sample, self.k, sorted=True)[0][:,-1], -1)\n",
    "        y = tf.cast(tf.greater_equal(gumbel_softmax_sample, threshold), tf.float32)\n",
    "        return y\n",
    "    \n",
    "    @tf.function\n",
    "    def sample_gumbel_k(self, shape):\n",
    "        \n",
    "        s = tf.map_fn(fn=lambda t: tf.random.gamma(shape, 1.0/self.k, t/self.k), \n",
    "                  elems=tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]))   \n",
    "        # now add the samples\n",
    "        s = tf.reduce_sum(s, 0)\n",
    "        # the log(m) term\n",
    "        s = s - tf.math.log(10.0)\n",
    "        # divide by k --> each s[c] has k samples whose sum is distributed as Gumbel(0, 1)\n",
    "        s = self._tau * (s / self.k)\n",
    "\n",
    "        return s\n",
    "    \n",
    "    #@tf.function\n",
    "    def sample_discrete_2(self, logits): \n",
    "        self.samples = self.sample_gumbel_k(tf.shape(logits))\n",
    "        gumbel_softmax_sample = logits + self.samples\n",
    "        threshold = tf.expand_dims(tf.nn.top_k(gumbel_softmax_sample, self.k, sorted=True)[0][:,-1], -1)\n",
    "        y = tf.cast(tf.greater_equal(gumbel_softmax_sample, threshold), tf.float32)\n",
    "        return y\n",
    "    \n",
    "    #@tf.function\n",
    "    def sample_discrete_2_reuse(self, logits):     \n",
    "        gumbel_softmax_sample = logits + self.samples\n",
    "        threshold = tf.expand_dims(tf.nn.top_k(gumbel_softmax_sample, self.k, sorted=True)[0][:,-1], -1)\n",
    "        y = tf.cast(tf.greater_equal(gumbel_softmax_sample, threshold), tf.float32)\n",
    "        return y\n",
    "    \n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def gumbel_topk_new(self, logits):\n",
    "\n",
    "        # we compute a map state for the distribution\n",
    "        # we also store the sample for later\n",
    "        z_train = self.sample_discrete_2(logits)        \n",
    "        threshold = tf.expand_dims(tf.nn.top_k(logits, self.k, sorted=True)[0][:,-1], -1)\n",
    "        z_test = tf.cast(tf.greater_equal(logits, threshold), tf.float32)\n",
    "        z_output = K.in_train_phase(z_train, z_test)\n",
    "        \n",
    "        def custom_grad(dy):\n",
    "\n",
    "            # we perturb (implicit diff) and then resuse sample for perturb and MAP\n",
    "            map_dy = self.sample_discrete_2_reuse(logits - (self._lambda*dy))\n",
    "            # we now compute the gradients as the difference (I-MLE gradients)\n",
    "            grad = tf.math.subtract(z_train, map_dy)\n",
    "            \n",
    "            # for the straight-through estimator, simply use the following line\n",
    "            #return dy, k\n",
    "            return grad\n",
    "\n",
    "        return z_output, custom_grad\n",
    "    \n",
    "    def call(self, logits):\n",
    "        \n",
    "        logits = tf.squeeze(logits, -1) # [batchsize, d]\n",
    "        y = self.gumbel_topk_new(logits)\n",
    "        y = tf.expand_dims(y, -1) #[batchsize, d, 1]\n",
    "        return y\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg\n",
    "\n",
    "\n",
    "def construct_gumbel_selector(X_ph, num_words, embedding_dims, maxlen):\n",
    "    \"\"\"\n",
    "    Build the L2X model for selecting words. \n",
    "\n",
    "    \"\"\"\n",
    "    emb_layer = Embedding(num_words, embedding_dims, weights=[embedding_matrix], input_length=maxlen, trainable=False, name='emb_gumbel')\n",
    "    emb = emb_layer(X_ph) #(350, 200) \n",
    "    #net = Dropout(0.2, name = 'dropout_gumbel')(emb)# this is not used in the L2X experiments\n",
    "    net = emb\n",
    "    first_layer = Conv1D(100, kernel_size, padding='same', activation='relu', strides=1, name = 'conv1_gumbel')(net)    \n",
    "\n",
    "    # global info\n",
    "    net_new = GlobalMaxPooling1D(name = 'new_global_max_pooling1d_1')(first_layer)\n",
    "    global_info = Dense(100, name = 'new_dense_1', activation='relu')(net_new) \n",
    "\n",
    "    # local info\n",
    "    net = Conv1D(100, 3, padding='same', activation='relu', strides=1, name = 'conv2_gumbel')(first_layer) \n",
    "    local_info = Conv1D(100, 3, padding='same', activation='relu', strides=1, name = 'conv3_gumbel')(net)  \n",
    "    combined = Concatenate()([global_info,local_info]) \n",
    "    net = Dropout(0.2, name = 'new_dropout_2')(combined)\n",
    "    net = Conv1D(100, 1, padding='same', activation='relu', strides=1, name = 'conv_last_gumbel')(net)   \n",
    "\n",
    "    logits_T = Conv1D(1, 1, padding='same', activation=None, strides=1, name = 'conv4_gumbel')(net)  \n",
    "    \n",
    "    return logits_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating model...')\n",
    "\n",
    "list_test_loss = []\n",
    "list_subset_precision = []\n",
    "\n",
    "# here we can now iterate a few times to compute statistics\n",
    "for iterc in range(10):\n",
    "    \n",
    "    tf.random.set_seed(iterc)\n",
    "    np.random.seed(iterc)\n",
    "\n",
    "    # create a new validation/test split\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_val_both, Y_val_both, test_size=0.5, random_state=iterc)\n",
    "    \n",
    "    # P(S|X)\n",
    "    #with tf.variable_scope('selection_model'):\n",
    "    X_ph = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "    logits_T = construct_gumbel_selector(X_ph, max_features, embedding_dims, maxlen)\n",
    "\n",
    "    ###############################################################################\n",
    "    #### here we switch between the different methods #############################\n",
    "    # the standard L2X approach\n",
    "    #tau = 0.1\n",
    "    #T = Sample_Concrete(tau, select_k)(logits_T)\n",
    "\n",
    "    # the I-MLE approach (ours)\n",
    "    # tau = temperature\n",
    "    # lambda = implicit differentiation perturbation magnitude:  q(z; theta') with theta' = theta - lambda dL/dz\n",
    "    T = IMLESubsetkLayer(select_k, _tau=1.0, _lambda=1000.0)(logits_T)\n",
    "    \n",
    "    # the SoftSub relaxation\n",
    "    #tau = 0.5\n",
    "    #T = SampleSubset(tau, select_k)(logits_T)\n",
    "    ###############################################################################\n",
    "    ###############################################################################     \n",
    "\n",
    "    # q(X_S)\n",
    "    emb2 = Embedding(max_features, embedding_dims, input_length=maxlen, weights=[embedding_matrix], trainable=False)(X_ph)\n",
    "    net = Mean(Multiply()([emb2, T]))\n",
    "    net = Dense(hidden_dims)(net)\n",
    "    net = Activation('relu')(net)\n",
    "    preds = Dense(1, activation='sigmoid', name = 'new_dense')(net)\n",
    "\n",
    "    model = Model(inputs=X_ph, outputs=preds)\n",
    "    #plot_model(model, to_file='model_plot.pdf', show_shapes=True, show_layer_names=True)\n",
    "      \n",
    "    model.compile(loss=['mse'], optimizer='adam', metrics=['mse']) \n",
    "\n",
    "    filepath=\"models/l2x.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_mse', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint] \n",
    "    st = time.time()\n",
    "\n",
    "    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), callbacks=callbacks_list, epochs=20, batch_size=batch_size)\n",
    "    duration = time.time() - st\n",
    "    print('Training time is {}'.format(duration))\n",
    "\n",
    "    results = model.evaluate(X_test, Y_test, batch_size=100)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "    list_test_loss.append(results[0])\n",
    "    \n",
    "    modelTest = Model(inputs=X_ph, outputs=T)\n",
    "    modelTest.load_weights('models/l2x.hdf5', by_name=True)\n",
    "\n",
    "    subset_prec = subset_precision(modelTest)\n",
    "    list_subset_precision.append(subset_prec)\n",
    "    print(\"Subset precision: \" + str(subset_prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(list_subset_precision))\n",
    "print(np.std(list_subset_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(list_test_loss))\n",
    "print(np.std(list_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
