2021-01-31 10:15:36,018	WARNING import_thread.py:132 -- The remote function 'maprop.blackbox.dijkstra.solver' has been exported 100 times. It's possible that this warning is accidental, but this may indicate that the same remote function is being defined repeatedly from within many tasks and exported to all of the workers. This can be a performance issue and can be resolved by defining the remote function on the driver instead. See https://github.com/ray-project/ray/issues/6240 for more discussion.
No existing entry for trainer_params.mode.type
{}[type] = <class 'int'>(0)
No existing entry for trainer_params.mode.objective_type
{'type': 0}[objective_type] = <class 'str'>(normal)
No existing entry for trainer_params.mode.use_marginal
{'type': 0, 'objective_type': 'normal'}[use_marginal] = <class 'bool'>(True)
No existing entry for trainer_params.mode.nb_samples
{'type': 0, 'objective_type': 'normal', 'use_marginal': True}[nb_samples] = <class 'int'>(10)
No existing entry for trainer_params.mode.temperature
{'type': 0, 'objective_type': 'normal', 'use_marginal': True, 'nb_samples': 10}[temperature] = <class 'float'>(0.001)
No existing entry for trainer_params.mode.use_gamma
{'type': 0, 'objective_type': 'normal', 'use_marginal': True, 'nb_samples': 10, 'temperature': 0.001}[use_gamma] = <class 'bool'>(True)
No existing entry for trainer_params.mode.loss_type
{'type': 0, 'objective_type': 'normal', 'use_marginal': True, 'nb_samples': 10, 'temperature': 0.001, 'use_gamma': True}[loss_type] = <class 'str'>(normal)
No existing entry for trainer_params.mode.perturb_w
{'type': 0, 'objective_type': 'normal', 'use_marginal': True, 'nb_samples': 10, 'temperature': 0.001, 'use_gamma': True, 'loss_type': 'normal'}[perturb_w] = <class 'bool'>(False)
No existing entry for trainer_params.mode.perturb_w_prime
{'type': 0, 'objective_type': 'normal', 'use_marginal': True, 'nb_samples': 10, 'temperature': 0.001, 'use_gamma': True, 'loss_type': 'normal', 'perturb_w': False}[perturb_w_prime] = <class 'bool'>(True)
{'batch_size': 70, 'l1_regconst': 0.0, 'lambda_val': 20.0, 'lr_milestone_1': 30, 'lr_milestone_2': 40, 'model_params': {'arch_params': {}, 'model_name': 'CombResnet18'}, 'neighbourhood_fn': '8-grid', 'optimizer_name': 'Adam', 'optimizer_params': {'lr': 0.0005}, 'preload_batch': True, 'use_cuda': True, 'use_lr_scheduling': True, 'mode': {'type': 0, 'objective_type': 'normal', 'use_marginal': True, 'nb_samples': 10, 'temperature': 0.001, 'use_gamma': True, 'loss_type': 'normal', 'perturb_w': False, 'perturb_w_prime': True}}[lambda_val] = <class 'float'>(1.0)
{'data_dir': 'data/warcraft_shortest_path/12x12', 'evaluate_with_extra': False, 'normalize': True, 'use_local_path': False, 'use_test_set': True}[data_dir] = <class 'str'>(data/warcraft_shortest_path/30x30)
No existing entry for trainer_params.mode.type
{}[type] = <class 'int'>(0)
No existing entry for trainer_params.mode.objective_type
{'type': 0}[objective_type] = <class 'str'>(normal)
No existing entry for trainer_params.mode.use_marginal
{'type': 0, 'objective_type': 'normal'}[use_marginal] = <class 'bool'>(True)
No existing entry for trainer_params.mode.nb_samples
{'type': 0, 'objective_type': 'normal', 'use_marginal': True}[nb_samples] = <class 'int'>(10)
No existing entry for trainer_params.mode.temperature
{'type': 0, 'objective_type': 'normal', 'use_marginal': True, 'nb_samples': 10}[temperature] = <class 'float'>(0.001)
No existing entry for trainer_params.mode.use_gamma
{'type': 0, 'objective_type': 'normal', 'use_marginal': True, 'nb_samples': 10, 'temperature': 0.001}[use_gamma] = <class 'bool'>(True)
No existing entry for trainer_params.mode.loss_type
{'type': 0, 'objective_type': 'normal', 'use_marginal': True, 'nb_samples': 10, 'temperature': 0.001, 'use_gamma': True}[loss_type] = <class 'str'>(normal)
No existing entry for trainer_params.mode.perturb_w
{'type': 0, 'objective_type': 'normal', 'use_marginal': True, 'nb_samples': 10, 'temperature': 0.001, 'use_gamma': True, 'loss_type': 'normal'}[perturb_w] = <class 'bool'>(False)
No existing entry for trainer_params.mode.perturb_w_prime
{'type': 0, 'objective_type': 'normal', 'use_marginal': True, 'nb_samples': 10, 'temperature': 0.001, 'use_gamma': True, 'loss_type': 'normal', 'perturb_w': False}[perturb_w_prime] = <class 'bool'>(True)
{'batch_size': 70, 'l1_regconst': 0.0, 'lambda_val': 20.0, 'lr_milestone_1': 30, 'lr_milestone_2': 40, 'model_params': {'arch_params': {}, 'model_name': 'CombResnet18'}, 'neighbourhood_fn': '8-grid', 'optimizer_name': 'Adam', 'optimizer_params': {'lr': 0.0005}, 'preload_batch': True, 'use_cuda': True, 'use_lr_scheduling': True, 'mode': {'type': 0, 'objective_type': 'normal', 'use_marginal': True, 'nb_samples': 10, 'temperature': 0.001, 'use_gamma': True, 'loss_type': 'normal', 'perturb_w': False, 'perturb_w_prime': True}}[lambda_val] = <class 'float'>(1.0)
{'data_dir': 'data/warcraft_shortest_path/12x12', 'evaluate_with_extra': False, 'normalize': True, 'use_local_path': False, 'use_test_set': True}[data_dir] = <class 'str'>(data/warcraft_shortest_path/30x30)
{
    "evaluate_every": 5,
    "loader_params": {
        "data_dir": "data/warcraft_shortest_path/30x30",
        "evaluate_with_extra": false,
        "normalize": true,
        "use_local_path": false,
        "use_test_set": true
    },
    "model_dir": "results/warcraft_shortest_path_combresnet",
    "num_cpus": 40,
    "num_epochs": 50,
    "problem_type": "warcraft_shortest_path",
    "ray_params": {},
    "save_visualizations": false,
    "seed": 1,
    "trainer_name": "DijkstraMAP",
    "trainer_params": {
        "batch_size": 70,
        "l1_regconst": 0.0,
        "lambda_val": 1.0,
        "lr_milestone_1": 30,
        "lr_milestone_2": 40,
        "mode": {
            "loss_type": "normal",
            "nb_samples": 10,
            "objective_type": "normal",
            "perturb_w": false,
            "perturb_w_prime": true,
            "temperature": 0.001,
            "type": 0,
            "use_gamma": true,
            "use_marginal": true
        },
        "model_params": {
            "arch_params": {},
            "model_name": "CombResnet18"
        },
        "neighbourhood_fn": "8-grid",
        "optimizer_name": "Adam",
        "optimizer_params": {
            "lr": 0.0005
        },
        "preload_batch": true,
        "use_cuda": true,
        "use_lr_scheduling": true
    },
    "use_ray": true
}
MAP-BACKPROP MODE: {
    "loss_type": "normal",
    "nb_samples": 10,
    "objective_type": "normal",
    "perturb_w": false,
    "perturb_w_prime": true,
    "temperature": 0.001,
    "type": 0,
    "use_gamma": true,
    "use_marginal": true
}
True
META: {'input_image_size': 240, 'output_features': 900, 'num_channels': 3, 'denormalize': <maprop.decorators.input_to_numpy object at 0x2b4180fd6630>}
EVALUATING
{'loss': 53.552001876831056, 'accuracy': 0.9404978048801422, 'perfect_match_accuracy': 0.001, 'cost_ratio_suggested_true': 2.4892180275917055, 'below_10.0_percent_acc': 0.122, 'below_1.0_percent_acc': 0.012, 'below_0.1_percent_acc': 0.012, 'below_0.01_percent_acc': 0.012, 'below_0.001_percent_acc': 0.011, 'below_0.0001_percent_acc': 0.011, 'valid_acc': 1.0}
Epoch: 1	Batch time (4.086753)	Data time (0.109471)	Cuda time (0.000002)	Loss (31.234001)	Accuracy (0.965296)
Epoch: 2	Batch time (4.044047)	Data time (0.108233)	Cuda time (0.000002)	Loss (13.367501)	Accuracy (0.985147)
Epoch: 3	Batch time (4.036461)	Data time (0.110387)	Cuda time (0.000003)	Loss (10.268600)	Accuracy (0.988590)
Epoch: 4	Batch time (4.052642)	Data time (0.107123)	Cuda time (0.000003)	Loss (9.316800)	Accuracy (0.989648)
Epoch: 5	Batch time (3.993572)	Data time (0.105498)	Cuda time (0.000003)	Loss (8.005300)	Accuracy (0.991105)
EVALUATING
{'loss': 7.569000291824341, 'accuracy': 0.9915900260210038, 'perfect_match_accuracy': 0.494, 'cost_ratio_suggested_true': 1.0431397461891174, 'below_10.0_percent_acc': 0.998, 'below_1.0_percent_acc': 0.956, 'below_0.1_percent_acc': 0.917, 'below_0.01_percent_acc': 0.917, 'below_0.001_percent_acc': 0.882, 'below_0.0001_percent_acc': 0.882, 'valid_acc': 1.0}
Epoch: 6	Batch time (4.027592)	Data time (0.108077)	Cuda time (0.000002)	Loss (7.782800)	Accuracy (0.991352)
Epoch: 7	Batch time (3.980973)	Data time (0.105546)	Cuda time (0.000002)	Loss (6.806500)	Accuracy (0.992437)
Epoch: 8	Batch time (3.971579)	Data time (0.098591)	Cuda time (0.000003)	Loss (6.861600)	Accuracy (0.992376)
Epoch: 9	Batch time (3.947770)	Data time (0.097203)	Cuda time (0.000002)	Loss (6.332100)	Accuracy (0.992964)
Traceback (most recent call last):
  File "./cli/warcraft-cli.py", line 176, in <module>
    main()
  File "./cli/warcraft-cli.py", line 160, in main
    train_results = trainer.train_epoch()
  File "/home/pminervi/workspace/map-backprop/maprop/warcraft_shortest_path/trainers.py", line 112, in train_epoch
    loss.backward()
  File "/home/pminervi/anaconda3/envs/gpu/lib/python3.6/site-packages/torch/tensor.py", line 185, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pminervi/anaconda3/envs/gpu/lib/python3.6/site-packages/torch/autograd/__init__.py", line 127, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: [36mray::solver()[39m (pid=26902, ip=10.200.5.13)
  File "python/ray/_raylet.pyx", line 444, in ray._raylet.execute_task
  File "/home/pminervi/anaconda3/envs/gpu/lib/python3.6/site-packages/ray/memory_monitor.py", line 140, in raise_if_low_memory
    self.error_threshold))
ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node gonzo-605-13.local is used (59.83 / 62.66 GB). The top 10 memory consumers are:

PID	MEM	COMMAND
26013	15.73GiB	python3 ./cli/warcraft-cli.py settings/warcraft_shortest_path/12x12_map.json trainer_params.mode.typ
26418	11.7GiB	python main_t1_sep.py -jn c11_sbj60_lr1e-3 -ppn 11 --approach AnisoUnet -nts 2 -nf 16 -nk 2 -mt 2 -n
35466	10.56GiB	python3 ./cli/warcraft-cli.py settings/warcraft_shortest_path/12x12_map.json trainer_params.mode.typ
21872	5.51GiB	python test.py --model_path ../pretrained_models/triviaqa_large_dpr/ --test_data_path ../data/prepro
22054	5.5GiB	python test.py --model_path ../pretrained_models/triviaqa_large_dpr/ --test_data_path ../data/prepro
22051	5.5GiB	python test.py --model_path ../pretrained_models/triviaqa_large_dpr/ --test_data_path ../data/prepro
22052	5.5GiB	python test.py --model_path ../pretrained_models/triviaqa_large_dpr/ --test_data_path ../data/prepro
22053	5.5GiB	python test.py --model_path ../pretrained_models/triviaqa_large_dpr/ --test_data_path ../data/prepro
26938	0.08GiB	ray::IDLE
26937	0.08GiB	ray::IDLE

In addition, up to 0.13 GiB of shared memory is currently being used by the Ray object store. You can set the object store size with the `object_store_memory` parameter when starting Ray.
---
--- Tip: Use the `ray memory` command to list active objects in the cluster.
---
2021-01-31 11:45:27,982	ERROR worker.py:1037 -- Possible unhandled error from worker: [36mray::solver()[39m (pid=26914, ip=10.200.5.13)
  File "python/ray/_raylet.pyx", line 444, in ray._raylet.execute_task
  File "/home/pminervi/anaconda3/envs/gpu/lib/python3.6/site-packages/ray/memory_monitor.py", line 140, in raise_if_low_memory
    self.error_threshold))
ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node gonzo-605-13.local is used (59.83 / 62.66 GB). The top 10 memory consumers are:

PID	MEM	COMMAND
26013	15.73GiB	python3 ./cli/warcraft-cli.py settings/warcraft_shortest_path/12x12_map.json trainer_params.mode.typ
26418	11.7GiB	python main_t1_sep.py -jn c11_sbj60_lr1e-3 -ppn 11 --approach AnisoUnet -nts 2 -nf 16 -nk 2 -mt 2 -n
35466	10.56GiB	python3 ./cli/warcraft-cli.py settings/warcraft_shortest_path/12x12_map.json trainer_params.mode.typ
21872	5.51GiB	python test.py --model_path ../pretrained_models/triviaqa_large_dpr/ --test_data_path ../data/prepro
22054	5.5GiB	python test.py --model_path ../pretrained_models/triviaqa_large_dpr/ --test_data_path ../data/prepro
22051	5.5GiB	python test.py --model_path ../pretrained_models/triviaqa_large_dpr/ --test_data_path ../data/prepro
22052	5.5GiB	python test.py --model_path ../pretrained_models/triviaqa_large_dpr/ --test_data_path ../data/prepro
22053	5.5GiB	python test.py --model_path ../pretrained_models/triviaqa_large_dpr/ --test_data_path ../data/prepro
26938	0.08GiB	ray::IDLE
26937	0.08GiB	ray::IDLE

In addition, up to 0.13 GiB of shared memory is currently being used by the Ray object store. You can set the object store size with the `object_store_memory` parameter when starting Ray.
---
--- Tip: Use the `ray memory` command to list active objects in the cluster.
---
