{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Concatenate, Softmax, LayerNormalization, Dropout\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import functools\n",
    "\n",
    "def neighbours_8(x, y, x_max, y_max):\n",
    "    deltas_x = (-1, 0, 1)\n",
    "    deltas_y = (-1, 0, 1)\n",
    "    for (dx, dy) in itertools.product(deltas_x, deltas_y):\n",
    "        x_new, y_new = x + dx, y + dy\n",
    "        if 0 <= x_new < x_max and 0 <= y_new < y_max and (dx, dy) != (0, 0):\n",
    "            yield x_new, y_new\n",
    "\n",
    "\n",
    "def neighbours_4(x, y, x_max, y_max):\n",
    "    for (dx, dy) in [(1, 0), (0, 1), (0, -1), (-1, 0)]:\n",
    "        x_new, y_new = x + dx, y + dy\n",
    "        if 0 <= x_new < x_max and 0 <= y_new < y_max and (dx, dy) != (0, 0):\n",
    "            yield x_new, y_new\n",
    "\n",
    "\n",
    "def get_neighbourhood_func(neighbourhood_fn):\n",
    "    if neighbourhood_fn == \"4-grid\":\n",
    "        return neighbours_4\n",
    "    elif neighbourhood_fn == \"8-grid\":\n",
    "        return neighbours_8\n",
    "    else:\n",
    "        raise Exception(f\"neighbourhood_fn of {neighbourhood_fn} not possible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "import heapq\n",
    "\n",
    "#DijkstraOutput = namedtuple(\"DijkstraOutput\", [\"shortest_path\", \"is_unique\", \"transitions\"])\n",
    "\n",
    "\n",
    "def dijkstra(matrix, neighbourhood_fn=\"8-grid\", request_transitions=False):\n",
    "\n",
    "    x_max, y_max = matrix.shape\n",
    "    neighbors_func = partial(get_neighbourhood_func(neighbourhood_fn), x_max=x_max, y_max=y_max)\n",
    "\n",
    "    costs = np.full_like(matrix, 1.0e10)\n",
    "    costs[0][0] = matrix[0][0]\n",
    "    num_path = np.zeros_like(matrix)\n",
    "    num_path[0][0] = 1\n",
    "    priority_queue = [(matrix[0][0], (0, 0))]\n",
    "    certain = set()\n",
    "    transitions = dict()\n",
    "\n",
    "    while priority_queue:\n",
    "        cur_cost, (cur_x, cur_y) = heapq.heappop(priority_queue)\n",
    "        if (cur_x, cur_y) in certain:\n",
    "            pass\n",
    "\n",
    "        for x, y in neighbors_func(cur_x, cur_y):\n",
    "            if (x, y) not in certain:\n",
    "                if matrix[x][y] + costs[cur_x][cur_y] < costs[x][y]:\n",
    "                    costs[x][y] = matrix[x][y] + costs[cur_x][cur_y]\n",
    "                    heapq.heappush(priority_queue, (costs[x][y], (x, y)))\n",
    "                    transitions[(x, y)] = (cur_x, cur_y)\n",
    "                    num_path[x, y] = num_path[cur_x, cur_y]\n",
    "                elif matrix[x][y] + costs[cur_x][cur_y] == costs[x][y]:\n",
    "                    num_path[x, y] += 1\n",
    "\n",
    "        certain.add((cur_x, cur_y))\n",
    "    # retrieve the path\n",
    "    cur_x, cur_y = x_max - 1, y_max - 1\n",
    "    on_path = np.zeros_like(matrix)\n",
    "    on_path[-1][-1] = 1\n",
    "    while (cur_x, cur_y) != (0, 0):\n",
    "        cur_x, cur_y = transitions[(cur_x, cur_y)]\n",
    "        on_path[cur_x, cur_y] = 1.0\n",
    "\n",
    "    is_unique = num_path[-1, -1] == 1\n",
    "\n",
    "    return on_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# let's load the images of the grids\n",
    "train_prefix = \"train\"\n",
    "val_prefix = \"test\"\n",
    "data_suffix = \"maps\"\n",
    "true_weights_suffix = \"\"\n",
    "\n",
    "data_dir = \"/mnt/data-c305/mniepert/reason/12x12\"\n",
    "\n",
    "train_data_path = os.path.join(data_dir, train_prefix + \"_\" + data_suffix + \".npy\")\n",
    "\n",
    "if os.path.exists(train_data_path):\n",
    "    train_inputs = np.load(os.path.join(data_dir, train_prefix + \"_\" + data_suffix + \".npy\")).astype(np.float32)\n",
    "    train_labels = np.load(os.path.join(data_dir, train_prefix + \"_shortest_paths.npy\"))\n",
    "    train_true_weights = np.load(os.path.join(data_dir, train_prefix + \"_vertex_weights.npy\"))\n",
    "\n",
    "    train_inputs = train_inputs.transpose(0, 3, 1, 2)\n",
    "    mean, std = (\n",
    "        np.mean(train_inputs, axis=(0, 2, 3), keepdims=True),\n",
    "        np.std(train_inputs, axis=(0, 2, 3), keepdims=True),\n",
    "      )\n",
    "\n",
    "    train_inputs -= mean\n",
    "    train_inputs /= std\n",
    "    train_inputs = train_inputs.transpose(0, 2, 3, 1)\n",
    "\n",
    "    val_inputs = np.load(os.path.join(data_dir, val_prefix + \"_\" + data_suffix + \".npy\")).astype(np.float32)\n",
    "    val_labels = np.load(os.path.join(data_dir, val_prefix + \"_shortest_paths.npy\"))\n",
    "    val_true_weights = np.load(os.path.join(data_dir, val_prefix + \"_vertex_weights.npy\"))\n",
    "    \n",
    "    val_inputs = val_inputs.transpose(0, 3, 1, 2)\n",
    "    val_inputs -= mean\n",
    "    val_inputs /= std\n",
    "    val_inputs = val_inputs.transpose(0, 2, 3, 1)\n",
    "\n",
    "    train_labels = tf.cast(train_labels, tf.float32)\n",
    "    val_labels = tf.cast(val_labels, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.88818   ,  1.8510492 , -0.14565916],\n",
       "        [ 0.4456104 ,  1.7403947 , -0.27828017],\n",
       "        [ 0.4701976 ,  1.7127311 , -0.27828017],\n",
       "        ...,\n",
       "        [ 1.847081  ,  2.072358  ,  2.0028007 ],\n",
       "        [ 2.0437784 ,  2.2936668 ,  2.2149942 ],\n",
       "        [ 1.9454297 ,  2.1830125 ,  2.1088974 ]],\n",
       "\n",
       "       [[ 1.3553369 ,  1.1317954 , -0.46394953],\n",
       "        [ 0.4701976 ,  1.657404  , -0.30480435],\n",
       "        [ 0.4947848 ,  1.7403947 , -0.27828017],\n",
       "        ...,\n",
       "        [ 1.7979065 ,  2.0170307 ,  1.9497523 ],\n",
       "        [ 1.5520345 ,  1.7403947 ,  1.6845105 ],\n",
       "        [ 1.1094649 ,  1.2424499 ,  1.2070749 ]],\n",
       "\n",
       "       [[ 1.6749705 ,  0.93815017, -0.62309474],\n",
       "        [ 0.7406568 ,  1.4084315 , -0.41090113],\n",
       "        [ 0.4701976 ,  1.7680583 , -0.27828017],\n",
       "        ...,\n",
       "        [ 2.0683658 ,  2.3213305 ,  2.2415185 ],\n",
       "        [ 1.256988  ,  1.4084315 ,  1.3662201 ],\n",
       "        [ 0.34726158,  0.38487816,  0.38482478]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.1421273 ,  1.159459  , -0.5700463 ],\n",
       "        [ 2.092953  ,  1.1041318 , -0.62309474],\n",
       "        [ 2.092953  ,  1.1317954 , -0.59657055],\n",
       "        ...,\n",
       "        [ 0.4701976 ,  1.7127311 , -0.27828017],\n",
       "        [ 0.4947848 ,  1.7680583 , -0.25175595],\n",
       "        [ 0.4701976 ,  1.7403947 , -0.27828017]],\n",
       "\n",
       "       [[ 2.0683658 ,  1.1041318 , -0.62309474],\n",
       "        [ 2.092953  ,  1.1317954 , -0.59657055],\n",
       "        [ 2.1667144 ,  1.159459  , -0.5700463 ],\n",
       "        ...,\n",
       "        [ 0.4210232 ,  1.6850675 , -0.30480435],\n",
       "        [ 0.4210232 ,  1.657404  , -0.33132854],\n",
       "        [ 0.39643598,  1.657404  , -0.30480435]],\n",
       "\n",
       "       [[ 2.1175401 ,  1.1317954 , -0.59657055],\n",
       "        [ 2.1421273 ,  1.1317954 , -0.59657055],\n",
       "        [ 2.1175401 ,  1.1317954 , -0.59657055],\n",
       "        ...,\n",
       "        [ 0.4210232 ,  1.657404  , -0.30480435],\n",
       "        [ 0.39643598,  1.6020768 , -0.33132854],\n",
       "        [ 0.4456104 ,  1.7127311 , -0.27828017]]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10000, 12, 12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 1.2, 1.2],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 1.2, 1.2, 1.2],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 0.8, 0.8, 0.8, 0.8],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 0.8, 0.8, 0.8, 0.8],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 0.8, 0.8, 0.8, 0.8, 0.8],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 0.8, 0.8, 0.8, 0.8],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 0.8, 0.8, 0.8, 0.8],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 1.2, 1.2, 0.8],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 7.7, 1.2, 1.2, 1.2, 1.2],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 9.2, 9.2, 1.2],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 9.2, 1.2, 1.2],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 1.2, 1.2, 9.2, 1.2]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_true_weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12, 12), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12, 12), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 96, 96, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_inputs, train_labels)).shuffle(10000).batch(70)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_inputs, val_labels, val_true_weights)).batch(100)\n",
    "#for element in train_ds:\n",
    "#    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def perturb_and_map_gm(x, distributions, map_states, labels):\n",
    "                   \n",
    "    # here we would compute distribution (with perturb and map)\n",
    "    # or only map state. Here, this is passed precomputed for efficiency reasons\n",
    "    # ...\n",
    "    \n",
    "    def custom_grad(dy):\n",
    "        # reconstruct the y with steepest decent\n",
    "        dy_map = tf.cast(dy < 0, tf.float32)\n",
    "        # compute MLE gradients (note: the minus here is because of the model representation:\n",
    "        # we present a field with 1 if the path is there, but the weight should be smaller\n",
    "        grad = -tf.math.subtract(distributions, dy_map), distributions, map_states, labels\n",
    "        return grad\n",
    "        \n",
    "    return map_states, custom_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, filter_num, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            strides=stride,\n",
    "                                            padding=\"same\")\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            strides=1,\n",
    "                                            padding=\"same\")\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        if stride != 1:\n",
    "            self.downsample = tf.keras.Sequential()\n",
    "            self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                                       kernel_size=(1, 1),\n",
    "                                                       strides=stride))\n",
    "            self.downsample.add(tf.keras.layers.BatchNormalization())\n",
    "        else:\n",
    "            self.downsample = lambda x: x\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        residual = self.downsample(inputs)\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "def make_basic_block_layer(filter_num, blocks, stride=1):\n",
    "    res_block = tf.keras.Sequential()\n",
    "    res_block.add(BasicBlock(filter_num, stride=stride))\n",
    "\n",
    "    for _ in range(1, blocks):\n",
    "        res_block.add(BasicBlock(filter_num, stride=1))\n",
    "\n",
    "    return res_block\n",
    "\n",
    "      \n",
    "class ResNet18Inference(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet18Inference, self).__init__()\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=64,\n",
    "                                            kernel_size=(7, 7),\n",
    "                                            strides=2,\n",
    "                                            padding=\"same\",\n",
    "                                            use_bias=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\n",
    "                                               strides=2,\n",
    "                                               padding=\"same\")\n",
    "        self.layer1 = make_basic_block_layer(filter_num=64, blocks=2)\n",
    "\n",
    "        output_shape = (int(12), int(12))\n",
    "        self.adaptivepool = tfa.layers.AdaptiveAveragePooling2D(output_shape)\n",
    "\n",
    "        \n",
    "    def set_sample_matrix(self, samples_in):\n",
    "        self.samples = tf.transpose(tf.cast(samples_in, tf.float32))\n",
    "        print(self.samples.shape)\n",
    "        \n",
    "    def call(self, inputs, distributions, map_states, labels, training=None, mask=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.layer1(x, training=training)\n",
    "        x = self.adaptivepool(x)\n",
    "        x = tf.math.reduce_mean(x, axis=3)\n",
    "        \n",
    "        # compute shortest path based on current output\n",
    "        # at this point, no gradient flow into x_logit!\n",
    "        map_states = perturb_and_map_gm(x, distributions, map_states, labels)\n",
    "        \n",
    "        return x, map_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference = ResNet18Inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = model_inference(train_inputs[1:2], train_labels[1:2], train_labels[1:2], train_labels[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 12, 12), dtype=float32, numpy=\n",
       "array([[[0.25955933, 0.26167887, 0.26381123, 0.25844768, 0.26301086,\n",
       "         0.27391315, 0.31467497, 0.3418848 , 0.21724701, 0.18243653,\n",
       "         0.18278003, 0.18631047],\n",
       "        [0.25534493, 0.2727787 , 0.27320462, 0.26971462, 0.27585873,\n",
       "         0.29221272, 0.34953073, 0.2790551 , 0.1976625 , 0.19193222,\n",
       "         0.19042224, 0.18234345],\n",
       "        [0.25496253, 0.26967028, 0.2705915 , 0.26693794, 0.27539498,\n",
       "         0.32089794, 0.35915726, 0.24159817, 0.20265436, 0.19427195,\n",
       "         0.1846967 , 0.17933626],\n",
       "        [0.26002088, 0.26791874, 0.26488575, 0.26401204, 0.28512442,\n",
       "         0.35075635, 0.28211945, 0.21230415, 0.18272215, 0.16492745,\n",
       "         0.15580536, 0.14815216],\n",
       "        [0.2612709 , 0.2720647 , 0.26820254, 0.26552305, 0.289021  ,\n",
       "         0.3539951 , 0.24561048, 0.2212548 , 0.16635415, 0.16132115,\n",
       "         0.16106555, 0.15142912],\n",
       "        [0.26117513, 0.26973176, 0.26481432, 0.26415944, 0.2804047 ,\n",
       "         0.3713754 , 0.28572124, 0.20606565, 0.18371947, 0.16232169,\n",
       "         0.16189471, 0.1593667 ],\n",
       "        [0.2602365 , 0.27256167, 0.2708121 , 0.26849377, 0.27553135,\n",
       "         0.31164008, 0.3524342 , 0.23017047, 0.21550375, 0.20407492,\n",
       "         0.19737744, 0.16436258],\n",
       "        [0.25756162, 0.27073163, 0.27212542, 0.27279595, 0.27690512,\n",
       "         0.31861573, 0.40292037, 0.2917352 , 0.19348018, 0.17847478,\n",
       "         0.18653557, 0.20050532],\n",
       "        [0.25701618, 0.2649802 , 0.27035534, 0.2758299 , 0.28531003,\n",
       "         0.352722  , 0.36137605, 0.3422106 , 0.25741324, 0.2910631 ,\n",
       "         0.2797221 , 0.18734345],\n",
       "        [0.2529881 , 0.26239505, 0.27202383, 0.2754465 , 0.2783789 ,\n",
       "         0.3091827 , 0.34302998, 0.27350903, 0.2984489 , 0.35576773,\n",
       "         0.36577266, 0.22888517],\n",
       "        [0.2652414 , 0.27302524, 0.27678812, 0.28059947, 0.2851143 ,\n",
       "         0.32414347, 0.36265957, 0.24077806, 0.31068125, 0.36438632,\n",
       "         0.34047464, 0.19724295],\n",
       "        [0.2788775 , 0.2970056 , 0.29817474, 0.30022362, 0.3076385 ,\n",
       "         0.36705697, 0.28654936, 0.19418645, 0.23653808, 0.32591772,\n",
       "         0.3764086 , 0.24029166]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([12, 12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12, 12), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dijkstra(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HammingLoss(y_true, y_pred):\n",
    "    loss = tf.math.reduce_mean(y_pred * (tf.ones_like(y_true) - y_true) + (tf.ones_like(y_pred) - y_pred) * y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom accuracy function -- since there are several optimal cost solutions\n",
    "class SameSolutionAccuracy(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='same_solution_accuracy', **kwargs):\n",
    "        super(SameSolutionAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.same_solutions = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.counter = self.add_weight(name='counter', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, cost_matrix):\n",
    "        \n",
    "        y_true = tf.reshape(y_true, [-1, 144])\n",
    "        y_pred = tf.reshape(y_pred, [-1, 144])\n",
    "        cost_matrix = tf.cast(cost_matrix, tf.float32)\n",
    "        cost_matrix = tf.reshape(cost_matrix, [-1, 144])\n",
    "        \n",
    "        y_true_cost = tf.math.reduce_sum(cost_matrix * y_true, 1)\n",
    "        y_pred_cost = tf.math.reduce_sum(cost_matrix * y_pred, 1)\n",
    "\n",
    "        # True if the cost is the same\n",
    "        equal_values = tf.cast(tf.math.less_equal(y_pred_cost, y_true_cost), tf.float32)\n",
    "        #print(equal_values)\n",
    "        sum_correct_in_batch = tf.math.reduce_sum(equal_values)\n",
    "\n",
    "        self.same_solutions.assign_add(sum_correct_in_batch)\n",
    "        self.counter.assign_add(y_true.shape[0])\n",
    "\n",
    "    def result(self):\n",
    "        return self.same_solutions/self.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_samesol = SameSolutionAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, distributions, map_states, labels):  \n",
    "    with tf.GradientTape() as tape:\n",
    "        x, returned_map_state = model_inference(images, distributions, map_states, labels, training=True)     \n",
    "        loss = HammingLoss(labels, returned_map_state) \n",
    "        \n",
    "    gradients = tape.gradient(loss, model_inference.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model_inference.trainable_variables))\n",
    "\n",
    "    train_loss(HammingLoss(labels, returned_map_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def test_step(images, labels, cost_matrix):\n",
    "    predictions, _ = model_inference(images, labels, labels, labels, training=False)\n",
    "    #t_loss = loss_object_0(labels, predictions[0])\n",
    "\n",
    "    weight_matrix = predictions.numpy()\n",
    "    map_paths = np.zeros_like(weight_matrix)\n",
    "    for i in range(weight_matrix.shape[0]):\n",
    "        map_paths[i] = dijkstra(weight_matrix[i])\n",
    "        \n",
    "    test_samesol(labels, map_paths, cost_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.11603071540594101\n",
      "Epoch 2, Loss: 0.09519993513822556\n",
      "Epoch 3, Loss: 0.06260207295417786\n",
      "Epoch 4, Loss: 0.05724667012691498\n",
      "Epoch 5, Loss: 0.05523040518164635\n",
      "Epoch 6, Loss: 0.054028138518333435\n",
      "Epoch 7, Loss: 0.051278356462717056\n",
      "Epoch 8, Loss: 0.04958351328969002\n",
      "Epoch 9, Loss: 0.04872789606451988\n",
      "Epoch 10, Loss: 0.047853533178567886\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(10):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        \n",
    "        # first get the weights from the current model\n",
    "        x, _ = model_inference(images, labels, labels, labels, training=False)\n",
    "\n",
    "        # store the weights as a numpy matrix\n",
    "        weight_matrix = x.numpy()\n",
    "        \n",
    "        # distributions stores the probabilities of variables\n",
    "        distributions = np.zeros_like(weight_matrix)\n",
    "        # stores all the map states\n",
    "        map_states = np.zeros_like(weight_matrix)\n",
    "        \n",
    "        # here we iterate over elements in batch\n",
    "        for i in range(weight_matrix.shape[0]):\n",
    "            # first we add the MAP state to the array\n",
    "            map_path = dijkstra(weight_matrix[i])\n",
    "            map_states[i] = map_path\n",
    "            \n",
    "            #distributions[i] = distributions[i] + map_path\n",
    "            # we perturb the predictions 9 times\n",
    "            for j in range(10):\n",
    "                # gumbel perturbation on the weights\n",
    "                perturbed_matrix = weight_matrix[i] + np.random.logistic(0, 1, weight_matrix[i].shape)\n",
    "                # compute map path on the perturbed problem\n",
    "                distributions[i] = distributions[i] + dijkstra(perturbed_matrix)\n",
    "            distributions[i] = distributions[i] / 10.0\n",
    "            #print(distributions[i])\n",
    "        \n",
    "        #print(distributions)\n",
    "        train_step(images, distributions, map_states, labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}'\n",
    "    print(template.format(epoch + 1, train_loss.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SameSol: 0.5049999952316284\n"
     ]
    }
   ],
   "source": [
    "# compute the accuracy wrt optimal cost paths (like in paper)\n",
    "test_samesol.reset_states()\n",
    "\n",
    "for test_images, test_labels, cost_matrix in test_ds:\n",
    "    test_step(test_images, test_labels, cost_matrix)\n",
    "    \n",
    "template = 'SameSol: {}'\n",
    "print(template.format(test_samesol.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
