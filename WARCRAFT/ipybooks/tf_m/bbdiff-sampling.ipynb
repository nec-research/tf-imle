{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Concatenate, Softmax, LayerNormalization, Dropout\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import functools\n",
    "\n",
    "def neighbours_8(x, y, x_max, y_max):\n",
    "    deltas_x = (-1, 0, 1)\n",
    "    deltas_y = (-1, 0, 1)\n",
    "    for (dx, dy) in itertools.product(deltas_x, deltas_y):\n",
    "        x_new, y_new = x + dx, y + dy\n",
    "        if 0 <= x_new < x_max and 0 <= y_new < y_max and (dx, dy) != (0, 0):\n",
    "            yield x_new, y_new\n",
    "\n",
    "\n",
    "def neighbours_4(x, y, x_max, y_max):\n",
    "    for (dx, dy) in [(1, 0), (0, 1), (0, -1), (-1, 0)]:\n",
    "        x_new, y_new = x + dx, y + dy\n",
    "        if 0 <= x_new < x_max and 0 <= y_new < y_max and (dx, dy) != (0, 0):\n",
    "            yield x_new, y_new\n",
    "\n",
    "\n",
    "def get_neighbourhood_func(neighbourhood_fn):\n",
    "    if neighbourhood_fn == \"4-grid\":\n",
    "        return neighbours_4\n",
    "    elif neighbourhood_fn == \"8-grid\":\n",
    "        return neighbours_8\n",
    "    else:\n",
    "        raise Exception(f\"neighbourhood_fn of {neighbourhood_fn} not possible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "import heapq\n",
    "\n",
    "#DijkstraOutput = namedtuple(\"DijkstraOutput\", [\"shortest_path\", \"is_unique\", \"transitions\"])\n",
    "\n",
    "\n",
    "def dijkstra(matrix, neighbourhood_fn=\"8-grid\", request_transitions=False):\n",
    "\n",
    "    x_max, y_max = matrix.shape\n",
    "    neighbors_func = partial(get_neighbourhood_func(neighbourhood_fn), x_max=x_max, y_max=y_max)\n",
    "\n",
    "    costs = np.full_like(matrix, 1.0e10)\n",
    "    costs[0][0] = matrix[0][0]\n",
    "    num_path = np.zeros_like(matrix)\n",
    "    num_path[0][0] = 1\n",
    "    priority_queue = [(matrix[0][0], (0, 0))]\n",
    "    certain = set()\n",
    "    transitions = dict()\n",
    "\n",
    "    while priority_queue:\n",
    "        cur_cost, (cur_x, cur_y) = heapq.heappop(priority_queue)\n",
    "        if (cur_x, cur_y) in certain:\n",
    "            pass\n",
    "\n",
    "        for x, y in neighbors_func(cur_x, cur_y):\n",
    "            if (x, y) not in certain:\n",
    "                if matrix[x][y] + costs[cur_x][cur_y] < costs[x][y]:\n",
    "                    costs[x][y] = matrix[x][y] + costs[cur_x][cur_y]\n",
    "                    heapq.heappush(priority_queue, (costs[x][y], (x, y)))\n",
    "                    transitions[(x, y)] = (cur_x, cur_y)\n",
    "                    num_path[x, y] = num_path[cur_x, cur_y]\n",
    "                elif matrix[x][y] + costs[cur_x][cur_y] == costs[x][y]:\n",
    "                    num_path[x, y] += 1\n",
    "\n",
    "        certain.add((cur_x, cur_y))\n",
    "    # retrieve the path\n",
    "    cur_x, cur_y = x_max - 1, y_max - 1\n",
    "    on_path = np.zeros_like(matrix)\n",
    "    on_path[-1][-1] = 1\n",
    "    while (cur_x, cur_y) != (0, 0):\n",
    "        cur_x, cur_y = transitions[(cur_x, cur_y)]\n",
    "        on_path[cur_x, cur_y] = 1.0\n",
    "\n",
    "    is_unique = num_path[-1, -1] == 1\n",
    "\n",
    "    return on_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# let's load the images of the grids\n",
    "train_prefix = \"train\"\n",
    "val_prefix = \"test\"\n",
    "data_suffix = \"maps\"\n",
    "true_weights_suffix = \"\"\n",
    "\n",
    "data_dir = \"/mnt/data-c305/mniepert/reason/12x12\"\n",
    "\n",
    "train_data_path = os.path.join(data_dir, train_prefix + \"_\" + data_suffix + \".npy\")\n",
    "\n",
    "if os.path.exists(train_data_path):\n",
    "    train_inputs = np.load(os.path.join(data_dir, train_prefix + \"_\" + data_suffix + \".npy\")).astype(np.float32)\n",
    "    train_labels = np.load(os.path.join(data_dir, train_prefix + \"_shortest_paths.npy\"))\n",
    "    train_true_weights = np.load(os.path.join(data_dir, train_prefix + \"_vertex_weights.npy\"))\n",
    "\n",
    "    train_inputs = train_inputs.transpose(0, 3, 1, 2)\n",
    "    mean, std = (\n",
    "        np.mean(train_inputs, axis=(0, 2, 3), keepdims=True),\n",
    "        np.std(train_inputs, axis=(0, 2, 3), keepdims=True),\n",
    "      )\n",
    "\n",
    "    train_inputs -= mean\n",
    "    train_inputs /= std\n",
    "    train_inputs = train_inputs.transpose(0, 2, 3, 1)\n",
    "\n",
    "    val_inputs = np.load(os.path.join(data_dir, val_prefix + \"_\" + data_suffix + \".npy\")).astype(np.float32)\n",
    "    val_labels = np.load(os.path.join(data_dir, val_prefix + \"_shortest_paths.npy\"))\n",
    "    val_true_weights = np.load(os.path.join(data_dir, val_prefix + \"_vertex_weights.npy\"))\n",
    "    \n",
    "    val_inputs = val_inputs.transpose(0, 3, 1, 2)\n",
    "    val_inputs -= mean\n",
    "    val_inputs /= std\n",
    "    val_inputs = val_inputs.transpose(0, 2, 3, 1)\n",
    "\n",
    "    train_labels = tf.cast(train_labels, tf.float32)\n",
    "    val_labels = tf.cast(val_labels, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.88818   ,  1.8510492 , -0.14565916],\n",
       "        [ 0.4456104 ,  1.7403947 , -0.27828017],\n",
       "        [ 0.4701976 ,  1.7127311 , -0.27828017],\n",
       "        ...,\n",
       "        [ 1.847081  ,  2.072358  ,  2.0028007 ],\n",
       "        [ 2.0437784 ,  2.2936668 ,  2.2149942 ],\n",
       "        [ 1.9454297 ,  2.1830125 ,  2.1088974 ]],\n",
       "\n",
       "       [[ 1.3553369 ,  1.1317954 , -0.46394953],\n",
       "        [ 0.4701976 ,  1.657404  , -0.30480435],\n",
       "        [ 0.4947848 ,  1.7403947 , -0.27828017],\n",
       "        ...,\n",
       "        [ 1.7979065 ,  2.0170307 ,  1.9497523 ],\n",
       "        [ 1.5520345 ,  1.7403947 ,  1.6845105 ],\n",
       "        [ 1.1094649 ,  1.2424499 ,  1.2070749 ]],\n",
       "\n",
       "       [[ 1.6749705 ,  0.93815017, -0.62309474],\n",
       "        [ 0.7406568 ,  1.4084315 , -0.41090113],\n",
       "        [ 0.4701976 ,  1.7680583 , -0.27828017],\n",
       "        ...,\n",
       "        [ 2.0683658 ,  2.3213305 ,  2.2415185 ],\n",
       "        [ 1.256988  ,  1.4084315 ,  1.3662201 ],\n",
       "        [ 0.34726158,  0.38487816,  0.38482478]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.1421273 ,  1.159459  , -0.5700463 ],\n",
       "        [ 2.092953  ,  1.1041318 , -0.62309474],\n",
       "        [ 2.092953  ,  1.1317954 , -0.59657055],\n",
       "        ...,\n",
       "        [ 0.4701976 ,  1.7127311 , -0.27828017],\n",
       "        [ 0.4947848 ,  1.7680583 , -0.25175595],\n",
       "        [ 0.4701976 ,  1.7403947 , -0.27828017]],\n",
       "\n",
       "       [[ 2.0683658 ,  1.1041318 , -0.62309474],\n",
       "        [ 2.092953  ,  1.1317954 , -0.59657055],\n",
       "        [ 2.1667144 ,  1.159459  , -0.5700463 ],\n",
       "        ...,\n",
       "        [ 0.4210232 ,  1.6850675 , -0.30480435],\n",
       "        [ 0.4210232 ,  1.657404  , -0.33132854],\n",
       "        [ 0.39643598,  1.657404  , -0.30480435]],\n",
       "\n",
       "       [[ 2.1175401 ,  1.1317954 , -0.59657055],\n",
       "        [ 2.1421273 ,  1.1317954 , -0.59657055],\n",
       "        [ 2.1175401 ,  1.1317954 , -0.59657055],\n",
       "        ...,\n",
       "        [ 0.4210232 ,  1.657404  , -0.30480435],\n",
       "        [ 0.39643598,  1.6020768 , -0.33132854],\n",
       "        [ 0.4456104 ,  1.7127311 , -0.27828017]]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10000, 12, 12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 1.2, 1.2],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 1.2, 1.2, 1.2],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 0.8, 0.8, 0.8, 0.8],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 0.8, 0.8, 0.8, 0.8],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 0.8, 0.8, 0.8, 0.8, 0.8],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 0.8, 0.8, 0.8, 0.8],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 0.8, 0.8, 0.8, 0.8],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 1.2, 1.2, 0.8],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 7.7, 1.2, 1.2, 1.2, 1.2],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 9.2, 9.2, 1.2],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 9.2, 1.2, 1.2],\n",
       "       [7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 1.2, 1.2, 1.2, 1.2, 9.2, 1.2]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_true_weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12, 12), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12, 12), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 96, 96, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_inputs, train_labels)).shuffle(10000).batch(70)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_inputs, val_labels, val_true_weights)).batch(100)\n",
    "#for element in train_ds:\n",
    "#    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def perturb_and_map_gm(x, distributions, map_states, labels):\n",
    "                   \n",
    "    # here we would compute distribution (with perturb and map)\n",
    "    # or only map state. Here, this is passed precomputed for efficiency reasons\n",
    "    # ...\n",
    "    \n",
    "    def custom_grad(dy):\n",
    "        dy_map = tf.cast(dy < 0, tf.float32)\n",
    "        grad = -tf.math.subtract(distributions, dy_map), distributions, map_states, labels\n",
    "        return grad\n",
    "        \n",
    "    return map_states, custom_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, filter_num, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            strides=stride,\n",
    "                                            padding=\"same\")\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            strides=1,\n",
    "                                            padding=\"same\")\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        if stride != 1:\n",
    "            self.downsample = tf.keras.Sequential()\n",
    "            self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                                       kernel_size=(1, 1),\n",
    "                                                       strides=stride))\n",
    "            self.downsample.add(tf.keras.layers.BatchNormalization())\n",
    "        else:\n",
    "            self.downsample = lambda x: x\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        residual = self.downsample(inputs)\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "def make_basic_block_layer(filter_num, blocks, stride=1):\n",
    "    res_block = tf.keras.Sequential()\n",
    "    res_block.add(BasicBlock(filter_num, stride=stride))\n",
    "\n",
    "    for _ in range(1, blocks):\n",
    "        res_block.add(BasicBlock(filter_num, stride=1))\n",
    "\n",
    "    return res_block\n",
    "\n",
    "      \n",
    "class ResNet18Inference(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet18Inference, self).__init__()\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=64,\n",
    "                                            kernel_size=(7, 7),\n",
    "                                            strides=2,\n",
    "                                            padding=\"same\",\n",
    "                                            use_bias=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\n",
    "                                               strides=2,\n",
    "                                               padding=\"same\")\n",
    "        self.layer1 = make_basic_block_layer(filter_num=64, blocks=2)\n",
    "\n",
    "        output_shape = (int(12), int(12))\n",
    "        self.adaptivepool = tfa.layers.AdaptiveAveragePooling2D(output_shape)\n",
    "\n",
    "        \n",
    "    def set_sample_matrix(self, samples_in):\n",
    "        self.samples = tf.transpose(tf.cast(samples_in, tf.float32))\n",
    "        print(self.samples.shape)\n",
    "        \n",
    "    def call(self, inputs, distributions, map_states, labels, training=None, mask=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.layer1(x, training=training)\n",
    "        x = self.adaptivepool(x)\n",
    "        x = tf.math.reduce_mean(x, axis=3)\n",
    "        \n",
    "        # compute shortest path based on current output\n",
    "        # at this point, no gradient flow into x_logit!\n",
    "        map_states = perturb_and_map(x, distributions, map_states, labels)\n",
    "        \n",
    "        return x, map_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference = ResNet18Inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = model_inference(train_inputs[1:2], train_labels[1:2], train_labels[1:2], train_labels[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 12, 12), dtype=float32, numpy=\n",
       "array([[[0.23979837, 0.23264565, 0.23263429, 0.22638993, 0.23241958,\n",
       "         0.24237883, 0.29788095, 0.32114625, 0.2983883 , 0.29061562,\n",
       "         0.27851146, 0.29502916],\n",
       "        [0.24115965, 0.24500299, 0.24042656, 0.23586008, 0.24297407,\n",
       "         0.25716746, 0.32157487, 0.29084992, 0.28433514, 0.28513294,\n",
       "         0.28867802, 0.30894622],\n",
       "        [0.24231896, 0.24439242, 0.23696208, 0.23364656, 0.24358243,\n",
       "         0.29852647, 0.31885055, 0.29169732, 0.26031494, 0.23653966,\n",
       "         0.22138055, 0.22315218],\n",
       "        [0.24656403, 0.24032918, 0.23265685, 0.23091412, 0.24469167,\n",
       "         0.32106784, 0.28836292, 0.29939717, 0.23022737, 0.18923935,\n",
       "         0.17710221, 0.16660522],\n",
       "        [0.24980474, 0.24439427, 0.2353958 , 0.23173685, 0.24387415,\n",
       "         0.31358042, 0.29470038, 0.2902798 , 0.20873857, 0.18407404,\n",
       "         0.17859223, 0.16913651],\n",
       "        [0.24861898, 0.23942156, 0.23248366, 0.23105788, 0.24219738,\n",
       "         0.31644693, 0.31517464, 0.29794434, 0.23129934, 0.19075078,\n",
       "         0.18477455, 0.18792938],\n",
       "        [0.24757522, 0.24452753, 0.237362  , 0.2337599 , 0.24236792,\n",
       "         0.25035125, 0.3033083 , 0.28437406, 0.2771696 , 0.26437843,\n",
       "         0.2539199 , 0.20121351],\n",
       "        [0.24412338, 0.24538015, 0.238379  , 0.2367782 , 0.24869019,\n",
       "         0.27530786, 0.32543457, 0.31864527, 0.2810768 , 0.27809098,\n",
       "         0.29066476, 0.279586  ],\n",
       "        [0.24332446, 0.23662713, 0.23727736, 0.2420237 , 0.2521516 ,\n",
       "         0.3008285 , 0.35176602, 0.3055809 , 0.32469922, 0.38353053,\n",
       "         0.3548643 , 0.31014335],\n",
       "        [0.23817238, 0.23523769, 0.24055704, 0.24210696, 0.24340343,\n",
       "         0.25602186, 0.31363812, 0.2960189 , 0.39177185, 0.4382631 ,\n",
       "         0.41342926, 0.32027745],\n",
       "        [0.24438585, 0.23583293, 0.23799297, 0.24098131, 0.24394134,\n",
       "         0.29319602, 0.3238442 , 0.28994942, 0.3744058 , 0.40020987,\n",
       "         0.37614268, 0.31273183],\n",
       "        [0.26130673, 0.24722707, 0.2452637 , 0.24498528, 0.25357074,\n",
       "         0.30848712, 0.27274162, 0.26502156, 0.30259052, 0.39134085,\n",
       "         0.41767004, 0.3116467 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([12, 12])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12, 12), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dijkstra(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HammingLoss(y_true, y_pred):\n",
    "    loss = tf.math.reduce_mean(y_pred * (tf.ones_like(y_true) - y_true) + (tf.ones_like(y_pred) - y_pred) * y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom accuracy function\n",
    "class SameSolutionAccuracy(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='same_solution_accuracy', **kwargs):\n",
    "        super(SameSolutionAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.same_solutions = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.counter = self.add_weight(name='counter', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, cost_matrix):\n",
    "        \n",
    "        y_true = tf.reshape(y_true, [-1, 144])\n",
    "        y_pred = tf.reshape(y_pred, [-1, 144])\n",
    "        cost_matrix = tf.cast(cost_matrix, tf.float32)\n",
    "        cost_matrix = tf.reshape(cost_matrix, [-1, 144])\n",
    "        \n",
    "        #print(y_true.shape)\n",
    "        \n",
    "        y_true_cost = tf.math.reduce_sum(cost_matrix * y_true, 1)\n",
    "        y_pred_cost = tf.math.reduce_sum(cost_matrix * y_pred, 1)\n",
    "        \n",
    "        #print(y_true_cost)\n",
    "        #print(y_pred_cost)\n",
    "        \n",
    "        # True if the cost is the same\n",
    "        equal_values = tf.cast(tf.math.less_equal(y_pred_cost, y_true_cost), tf.float32)\n",
    "        #print(equal_values)\n",
    "        sum_correct_in_batch = tf.math.reduce_sum(equal_values)\n",
    "        #print(sum_correct_in_batch)\n",
    "        #print(y_true.shape[0])\n",
    "\n",
    "        self.same_solutions.assign_add(sum_correct_in_batch)\n",
    "        self.counter.assign_add(y_true.shape[0])\n",
    "        \n",
    "        \n",
    "        #print(\"---\")\n",
    "\n",
    "    def result(self):\n",
    "        return self.same_solutions/self.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_samesol = SameSolutionAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def train_step(images, distributions, map_states, labels):  \n",
    "    with tf.GradientTape() as tape:\n",
    "        x, _ = model_inference(images, distributions, map_states, labels, training=True)     \n",
    "        loss = HammingLoss(labels, map_states) \n",
    "        \n",
    "    gradients = tape.gradient(loss, model_inference.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model_inference.trainable_variables))\n",
    "\n",
    "    train_loss(HammingLoss(labels, map_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def test_step(images, labels, cost_matrix):\n",
    "    predictions, _ = model_inference(images, labels, labels, labels, training=False)\n",
    "    #t_loss = loss_object_0(labels, predictions[0])\n",
    "\n",
    "    weight_matrix = predictions.numpy()\n",
    "    map_paths = np.zeros_like(weight_matrix)\n",
    "    for i in range(weight_matrix.shape[0]):\n",
    "        map_paths[i] = dijkstra(weight_matrix[i])\n",
    "        \n",
    "    test_samesol(labels, map_paths, cost_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(12, 12) dtype=float32, numpy=\n",
      "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(12, 12) dtype=float32, numpy=\n",
      "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.]], dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[-0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444\n",
      "   0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444]\n",
      " [-0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444\n",
      "   0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444]\n",
      " [ 0.00694444 -0.00694444  0.00694444  0.00694444  0.00694444  0.00694444\n",
      "   0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444]\n",
      " [ 0.00694444  0.00694444 -0.00694444  0.00694444  0.00694444  0.00694444\n",
      "   0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444]\n",
      " [ 0.00694444  0.00694444  0.00694444 -0.00694444  0.00694444  0.00694444\n",
      "   0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444]\n",
      " [ 0.00694444  0.00694444  0.00694444  0.00694444 -0.00694444  0.00694444\n",
      "   0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444]\n",
      " [ 0.00694444  0.00694444  0.00694444  0.00694444  0.00694444 -0.00694444\n",
      "   0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444]\n",
      " [ 0.00694444  0.00694444  0.00694444  0.00694444  0.00694444 -0.00694444\n",
      "   0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444]\n",
      " [ 0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444\n",
      "  -0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444]\n",
      " [ 0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444\n",
      "   0.00694444 -0.00694444  0.00694444  0.00694444  0.00694444  0.00694444]\n",
      " [ 0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444\n",
      "   0.00694444  0.00694444 -0.00694444  0.00694444 -0.00694444  0.00694444]\n",
      " [ 0.00694444  0.00694444  0.00694444  0.00694444  0.00694444  0.00694444\n",
      "   0.00694444  0.00694444  0.00694444 -0.00694444  0.00694444 -0.00694444]], shape=(12, 12), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss_mse = tf.keras.losses.MeanSquaredError()\n",
    "def HammingLossMSE(y_true, y_pred, cost_matrix):\n",
    "    y_true = tf.reshape(y_true, [-1, 144])\n",
    "    y_pred = tf.reshape(y_pred, [-1, 144])\n",
    "    cost_matrix = tf.cast(cost_matrix, tf.float32)\n",
    "    cost_matrix = tf.reshape(cost_matrix, [-1, 144])\n",
    "        \n",
    "    y_true_cost = tf.math.reduce_sum(cost_matrix * y_true, 1)\n",
    "    y_pred_cost = tf.math.reduce_sum(cost_matrix * y_pred, 1)\n",
    "    loss = tf.math.square(tf.math.subtract(y_true_cost, y_pred_cost))\n",
    "    return loss\n",
    "\n",
    "predictions, _ = model_inference(train_inputs[0:1], train_labels[0:1], train_labels[0:1], train_labels[0:1], training=False)\n",
    "true_weights = train_true_weights[0:1]\n",
    "true_labels = tf.Variable(train_labels[0])\n",
    "\n",
    "#print(sum(sum(true_weights[0]*train_labels[0])))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    \n",
    "    weights = predictions[0].numpy()\n",
    "    map_path = dijkstra(weights)\n",
    "    map_path = tf.Variable(map_path)\n",
    "    \n",
    "    print(map_path)\n",
    "    print(true_labels)\n",
    "     \n",
    "    loss = HammingLoss(true_labels, map_path)\n",
    "    var_grad = tape.gradient(loss, [map_path, true_labels])\n",
    "    print(var_grad[0])\n",
    "    #print(np.sign(-var_grad.numpy()))\n",
    "    \n",
    "    #theta = np.sign(-var_grad.numpy())\n",
    "    #print(theta)\n",
    "    #for k in range(20):\n",
    "    #    map_path = dijkstra(weights + (k*theta))\n",
    "        #print(map_path)\n",
    "        #print(labels)\n",
    "        #print(sum(sum(map_path * weights)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.01045193336904049\n",
      "Epoch 2, Loss: 0.009578613564372063\n",
      "Epoch 3, Loss: 0.00895643513649702\n",
      "Epoch 4, Loss: 0.009052752517163754\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-2d84a84ff725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# first we add the MAP state to the array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mmap_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdijkstra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mmap_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m#map_path_length = np.count_nonzero(map_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2405c9cfa106>\u001b[0m in \u001b[0;36mdijkstra\u001b[0;34m(matrix, neighbourhood_fn, request_transitions)\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcur_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mnum_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0;32melif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_y\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0mnum_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(10):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        \n",
    "        # first get the weights from the current model\n",
    "        x, _ = model_inference(images, labels, labels, labels, training=False)\n",
    "\n",
    "        # store the weights as a numpy matrix\n",
    "        weight_matrix = x.numpy()\n",
    "        \n",
    "        # distributions stores the probabilities of variables\n",
    "        distributions = np.zeros_like(weight_matrix)\n",
    "        # stores all the map states\n",
    "        map_states = np.zeros_like(weight_matrix)\n",
    "        \n",
    "        # here we iterate over batches\n",
    "        for i in range(weight_matrix.shape[0]):\n",
    "            # first we add the MAP state to the array\n",
    "            map_path = dijkstra(weight_matrix[i])\n",
    "            map_states[i] = map_path\n",
    "            #map_path_length = np.count_nonzero(map_path)\n",
    "            distributions[i] = distributions[i] + map_path\n",
    "            # we perturb the predictions 9 times\n",
    "            #for j in range(5):\n",
    "            #    perturbed_matrix = weight_matrix[i] + (np.random.logistic(0, 1, weight_matrix[i].shape)/map_path_length)\n",
    "            #    distributions[i] = distributions[i] + dijkstra(perturbed_matrix)\n",
    "            #distributions[i] = distributions[i] / 5\n",
    "            #print(distributions[i])\n",
    "        \n",
    "        #print(distributions)\n",
    "        train_step(images, distributions, map_states, labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}'\n",
    "    print(template.format(epoch + 1, train_loss.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun dijkstra(perturbed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SameSol: 0.9480000138282776\n"
     ]
    }
   ],
   "source": [
    "test_samesol.reset_states()\n",
    "\n",
    "for test_images, test_labels, cost_matrix in test_ds:\n",
    "    test_step(test_images, test_labels, cost_matrix)\n",
    "    \n",
    "template = 'SameSol: {}'\n",
    "print(template.format(test_samesol.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model(val_inputs[0:2])\n",
    "x[1]\n",
    "#1-dijkstra(x[1]).shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
